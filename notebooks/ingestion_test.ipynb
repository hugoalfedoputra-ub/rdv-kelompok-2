{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f7fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import couchdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import requests\n",
    "import json\n",
    "import traceback\n",
    "from r2client.R2Client import R2Client as r2\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70ddc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_RDV_ROOT = \"rdv/\"\n",
    "\n",
    "ACCOUNT_ID = os.getenv(\"ACCOUNT_ID\")\n",
    "AWS_SECRET_KEY_ID = os.getenv(\"AWS_SECRET_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "COUCHDB_HOST = os.getenv(\"COUCHDB_HOST\")\n",
    "COUCHDB_PORT = os.getenv(\"COUCHDB_PORT\")\n",
    "COUCHDB_URL = f\"http://{COUCHDB_HOST}:{COUCHDB_PORT}\"\n",
    "COUCHDB_USERNAME = os.getenv(\"COUCHDB_USERNAME\")\n",
    "COUCHDB_PASSWORD = os.getenv(\"COUCHDB_PASSWORD\")\n",
    "AUTH = (COUCHDB_USERNAME, COUCHDB_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687cecc",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d781e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_s3():\n",
    "    try:\n",
    "        s3 = r2(\n",
    "                access_key=AWS_SECRET_KEY_ID,\n",
    "                secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "                endpoint=f\"https://{ACCOUNT_ID}.r2.cloudflarestorage.com\",\n",
    "        )\n",
    "        print(\"S3 client for Cloudflare R2 successfully created\")\n",
    "        return s3\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "def _make_couchdb_request(method, url, auth=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Makes an HTTP request to CouchDB and handles common errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.request(method, url, auth=auth, **kwargs)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4XX or 5XX)\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e.response.status_code} {e.response.reason}\")\n",
    "        try:\n",
    "            print(f\"CouchDB Error: {e.response.json()}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"CouchDB Response (not JSON): {e.response.text}\")\n",
    "        return None\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Connection Error: Could not connect to CouchDB at {url}. Details: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_couchdb_index(db_name, index_fields, ddoc_name=None, index_name=None, index_type=\"json\"):\n",
    "    \"\"\"\n",
    "    Creates a Mango query index in a CouchDB database.\n",
    "\n",
    "    Args:\n",
    "        db_name (str): The name of the database.\n",
    "        index_fields (list): A list of field names to include in the index.\n",
    "                             e.g., [\"name\", \"age\"]\n",
    "        ddoc_name (str, optional): The name of the design document.\n",
    "                                   Defaults to \"ddoc_<first_field>\".\n",
    "        index_name (str, optional): The name of the index.\n",
    "                                    Defaults to \"idx_<first_field>\".\n",
    "        index_type (str, optional): The type of index, usually \"json\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from CouchDB if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    if not index_fields:\n",
    "        print(\"Error: index_fields list cannot be empty.\")\n",
    "        return None\n",
    "\n",
    "    url = f\"{COUCHDB_URL}/{db_name}/_index\"\n",
    "\n",
    "    if ddoc_name is None:\n",
    "        ddoc_name = f\"ddoc_{index_fields[0].lower().replace('.', '_')}\" # Sanitize field name for ddoc\n",
    "    if index_name is None:\n",
    "        index_name = f\"idx_{'_'.join(f.lower().replace('.', '_') for f in index_fields)}\"\n",
    "\n",
    "    payload = {\n",
    "        \"index\": {\"fields\": index_fields},\n",
    "        \"ddoc\": ddoc_name,\n",
    "        \"name\": index_name,\n",
    "        \"type\": index_type\n",
    "    }\n",
    "\n",
    "    print(f\"Attempting to create index '{index_name}' in ddoc '{ddoc_name}' for fields {index_fields} in database '{db_name}'...\")\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response_data = _make_couchdb_request(\"POST\", url, auth=AUTH, json=payload, headers=headers)\n",
    "\n",
    "    if response_data:\n",
    "        if response_data.get(\"result\") == \"created\":\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "        elif response_data.get(\"result\") == \"exists\":\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "        else:\n",
    "            print(f\"Index creation status: {response_data.get('result', 'unknown')}\")\n",
    "    return response_data\n",
    "\n",
    "def query_couchdb_documents(db_name, selector, limit=100, fields=None, sort=None, use_index=None):\n",
    "    \"\"\"\n",
    "    Queries documents in a CouchDB database using Mango _find.\n",
    "\n",
    "    Args:\n",
    "        db_name (str): The name of the database.\n",
    "        selector (dict): The Mango query selector.\n",
    "                         e.g., {\"name\": \"Alice\"} for documents where name is Alice\n",
    "                         e.g., {} to select all documents (respecting limit)\n",
    "        limit (int, optional): The maximum number of documents to return. Defaults to 100.\n",
    "        fields (list, optional): A list of fields to return for each document.\n",
    "                                 If None, all fields are returned.\n",
    "        sort (list, optional): A list of sort criteria.\n",
    "                               e.g., [{\"name\": \"asc\"}]\n",
    "        use_index (str or list, optional): Hint to CouchDB to use a specific index.\n",
    "                                           Can be a design doc name (e.g., \"ddoc_name\")\n",
    "                                           or a list [ddoc_name, index_name].\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents matching the query, or an empty list if no matches or error.\n",
    "    \"\"\"\n",
    "    url = f\"{COUCHDB_URL}/{db_name}/_find\"\n",
    "    payload = {\n",
    "        \"selector\": selector,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    if fields:\n",
    "        payload[\"fields\"] = fields\n",
    "    if sort:\n",
    "        payload[\"sort\"] = sort\n",
    "    if use_index:\n",
    "        payload[\"use_index\"] = use_index\n",
    "\n",
    "    print(f\"Querying database '{db_name}' with selector: {selector}, limit: {limit}...\")\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response_data = _make_couchdb_request(\"POST\", url, auth=AUTH, json=payload, headers=headers)\n",
    "\n",
    "    if response_data and \"docs\" in response_data:\n",
    "        print(f\"Found {len(response_data['docs'])} documents.\")\n",
    "        return response_data[\"docs\"]\n",
    "    else:\n",
    "        print(\"No documents found or an error occurred during query.\")\n",
    "        return []\n",
    "\n",
    "def get_all_couchdb_docs_paginated(db_name, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Generator function to fetch all documents from a CouchDB database in batches.\n",
    "\n",
    "    Args:\n",
    "        db_name (str): The name of the database.\n",
    "        batch_size (int): The number of documents to fetch per request.\n",
    "\n",
    "    Yields:\n",
    "        dict: Individual documents from the database.\n",
    "    \"\"\"\n",
    "    print(f\"Starting to fetch all documents from '{db_name}' in batches of {batch_size}...\")\n",
    "    start_key = None\n",
    "    total_docs_fetched = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"include_docs\": \"true\",\n",
    "            \"limit\": batch_size\n",
    "        }\n",
    "        if start_key:\n",
    "            # When using start_key, CouchDB includes the document with that key.\n",
    "            # To avoid processing it again, we fetch limit + 1 and skip the first\n",
    "            # if it's the same as the previous last_key, OR use skip=1.\n",
    "            # A common pattern is to fetch `limit` and then use the ID of the last doc\n",
    "            # as the `start_key` for the next batch, with `skip=1`.\n",
    "            params[\"startkey\"] = json.dumps(start_key) # Needs to be JSON encoded string\n",
    "            params[\"skip\"] = 1 # Skip the start_key doc itself in the new batch\n",
    "\n",
    "        url = f\"{COUCHDB_URL}/{db_name}/_all_docs\"\n",
    "        response_data = _make_couchdb_request(\"GET\", url, auth=AUTH, params=params)\n",
    "\n",
    "        if not response_data or \"rows\" not in response_data:\n",
    "            print(\"Failed to fetch documents or no rows in response.\")\n",
    "            break\n",
    "\n",
    "        rows = response_data[\"rows\"]\n",
    "        if not rows:\n",
    "            print(\"No more documents found.\")\n",
    "            break\n",
    "\n",
    "        docs_in_batch = 0\n",
    "        for row in rows:\n",
    "            if \"doc\" in row and row[\"doc\"]: # Ensure the doc exists and is not deleted marker\n",
    "                # Skip if the doc is a design document, unless explicitly needed\n",
    "                if not row[\"id\"].startswith(\"_design/\"):\n",
    "                    yield row[\"doc\"]\n",
    "                    docs_in_batch += 1\n",
    "                    total_docs_fetched +=1\n",
    "            # If skip=1 was not used, and this is not the first page,\n",
    "            # and row['id'] == start_key, then we'd skip it here.\n",
    "            # But with skip=1, this check isn't strictly needed.\n",
    "\n",
    "        print(f\"  Fetched {docs_in_batch} documents in this batch. Total fetched: {total_docs_fetched}\")\n",
    "\n",
    "        if docs_in_batch < batch_size : # If we fetched fewer docs than requested (excluding skip effect)\n",
    "             # this means we've reached the end.\n",
    "             # When using skip=1, if docs_in_batch is 0, it's the end.\n",
    "             # If docs_in_batch < batch_size, it's also likely the end.\n",
    "            if start_key and docs_in_batch == 0: # if we used skip=1 and got 0 results\n",
    "                print(\"Reached end of documents (skip=1 returned 0).\")\n",
    "                break\n",
    "            if docs_in_batch < (batch_size if start_key else batch_size): # Heuristic\n",
    "                 print(f\"Fetched {docs_in_batch} which is less than batch_size {batch_size}, assuming end of data.\")\n",
    "                 break\n",
    "\n",
    "\n",
    "        # Set start_key for the next iteration to the ID of the last document fetched in this batch\n",
    "        start_key = rows[-1][\"id\"]\n",
    "\n",
    "    print(f\"Finished fetching. Total documents processed: {total_docs_fetched}\")\n",
    "\n",
    "def write_docs_to_csv(documents_iterable, csv_filepath, fieldnames_mapping):\n",
    "    \"\"\"\n",
    "    Writes documents from an iterable to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        documents_iterable: An iterable (e.g., a generator) yielding document dictionaries.\n",
    "        csv_filepath (str): The path to the output CSV file.\n",
    "        fieldnames_mapping (dict): A dictionary where keys are CSV header names\n",
    "                                   and values are functions (or simple string keys)\n",
    "                                   to extract data from the document.\n",
    "                                   e.g., {\"City\": lambda doc: doc.get(\"location\", {}).get(\"name\")}\n",
    "                                   e.g., {\"Provider\": \"provider\"}\n",
    "    \"\"\"\n",
    "    print(f\"Writing documents to CSV: {csv_filepath}\")\n",
    "    count = 0\n",
    "    with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        # Use the keys from fieldnames_mapping as the CSV headers\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=list(fieldnames_mapping.keys()))\n",
    "        writer.writeheader()\n",
    "\n",
    "        for doc in documents_iterable:\n",
    "            row_data = {}\n",
    "            for csv_header, accessor in fieldnames_mapping.items():\n",
    "                if callable(accessor):\n",
    "                    try:\n",
    "                        row_data[csv_header] = accessor(doc)\n",
    "                    except Exception as e:\n",
    "                        # print(f\"Warning: Error accessing data for header '{csv_header}' in doc '{doc.get('_id')}': {e}\")\n",
    "                        row_data[csv_header] = None # Or some other placeholder\n",
    "                elif isinstance(accessor, str):\n",
    "                    # Simple direct key access, handle potential KeyError\n",
    "                    keys = accessor.split('.')\n",
    "                    value = doc\n",
    "                    try:\n",
    "                        for key in keys:\n",
    "                            value = value[key]\n",
    "                        row_data[csv_header] = value\n",
    "                    except (KeyError, TypeError):\n",
    "                        # print(f\"Warning: Key '{accessor}' not found or path invalid in doc '{doc.get('_id')}'\")\n",
    "                        row_data[csv_header] = None # Or some other placeholder\n",
    "                else:\n",
    "                    row_data[csv_header] = None # Should not happen with correct mapping\n",
    "\n",
    "            writer.writerow(row_data)\n",
    "            count += 1\n",
    "            if count % 500 == 0: # Log progress\n",
    "                print(f\"  Written {count} rows to CSV...\")\n",
    "\n",
    "    print(f\"Successfully wrote {count} rows to {csv_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdf027",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b51c54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Server 'http://10.34.100.114:5984'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server = couchdb.Server(f\"http://{COUCHDB_USERNAME}:{COUCHDB_PASSWORD}@{COUCHDB_HOST}:{COUCHDB_PORT}\")\n",
    "server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d000222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Database 'free_weather'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = server['free_weather']\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549e9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Database 's3_metadata'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_md = server['s3_metadata']\n",
    "db_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb5cdc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 client for Cloudflare R2 successfully created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<r2client.R2Client.R2Client at 0x1a1abb51d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3: r2 = connect_to_s3()\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edf2fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create index 'data-file-name' in ddoc 'ddoc_data_file_name' for fields ['data_file_name'] in database 's3_metadata'...\n",
      "Index 'data-file-name' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': 'exists',\n",
       " 'id': '_design/ddoc_data_file_name',\n",
       " 'name': 'data-file-name'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_couchdb_index(db_md.name, index_fields=[\"data_file_name\"], index_name=\"data-file-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0ad7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document '393cd91c7b1e4c78a97314bef8595db0'@'1-b90960d1e7d7a7d60fbd172f3c6ed457' {'descriptor_file_name': '-7_9789104_112_59608_descriptor_2025-05-19_18_13_02_157939.csv', 'data_file_name': '-7_9789104_112_59608_data_2025-05-19_18_13_02_157939.csv'}>\n",
      "<Document 'f59b3c22c9524990b430cfc062a6cc6f'@'1-aef90468463e3aaeb7c7c15648d2e414' {'descriptor_file_name': '-7_9789104_112_59608_descriptor_2025-05-19_18_07_52_065567.csv', 'data_file_name': '-7_9789104_112_59608_data_2025-05-19_18_07_52_065567.csv'}>\n",
      "<Document '21759c594ce749dfb5278b3457983007'@'1-b0dfb873201dea84825d697ce8c39d56' {'descriptor_file_name': '-7_9789104_112_59608_descriptor_2025-05-19_17_56_31_459242.csv', 'data_file_name': '-7_9789104_112_59608_data_2025-05-19_17_56_31_459242.csv'}>\n"
     ]
    }
   ],
   "source": [
    "mango = {\n",
    "    \"selector\": {},\n",
    "    \"sort\": [{\"data_file_name\": \"desc\"}]\n",
    "}\n",
    "\n",
    "for row in db_md.find(mango):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2660bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file_name = '-7_9789104_112_59608_data_2025-05-19_18_13_02_157939.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a171f644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File rdv/-7_9789104_112_59608_data_2025-05-19_18_13_02_157939.csv downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "s3.download_file('work', S3_RDV_ROOT + latest_file_name, '../data/localcopy_' + latest_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31c0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175344 entries, 0 to 175343\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   datetime                    175344 non-null  object \n",
      " 1   temperature_2m              175328 non-null  float64\n",
      " 2   is_day                      175344 non-null  int64  \n",
      " 3   relative_humidity_2m        175328 non-null  float64\n",
      " 4   dew_point_2m                175328 non-null  float64\n",
      " 5   apparent_temperature        175328 non-null  float64\n",
      " 6   precipitation               175328 non-null  float64\n",
      " 7   weather_code                175328 non-null  float64\n",
      " 8   pressure_msl                175328 non-null  float64\n",
      " 9   surface_pressure            175328 non-null  float64\n",
      " 10  cloud_cover                 175328 non-null  float64\n",
      " 11  cloud_cover_low             175328 non-null  float64\n",
      " 12  cloud_cover_mid             175328 non-null  float64\n",
      " 13  cloud_cover_high            175328 non-null  float64\n",
      " 14  et0_fao_evapotranspiration  175328 non-null  float64\n",
      " 15  vapour_pressure_deficit     175328 non-null  float64\n",
      " 16  wind_speed_10m              175328 non-null  float64\n",
      " 17  wind_direction_10m          175328 non-null  float64\n",
      " 18  wind_gusts_10m              175328 non-null  float64\n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 25.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/localcopy_' + latest_file_name)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753d994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_MAPPING = {\n",
    "    \"id\": \"_id\",\n",
    "    \"provider\": \"provider\",\n",
    "    \"location_name\": \"location.name\",\n",
    "    \"region\": \"location.region\",\n",
    "    \"country\": \"location.country\",\n",
    "    \"lat\": \"location.lat\",\n",
    "    \"lon\": \"location.lon\",\n",
    "    \"localtime\": \"location.localtime\",\n",
    "    \"provider_last_updated\": \"current_weather.last_updated\",\n",
    "    \"temp_c\": \"current_weather.temp_c\",\n",
    "    \"temp_f\": \"current_weather.temp_f\",\n",
    "    \"is_day\": \"current_weather.is_day\",\n",
    "    \"weather_desc\": \"current_weather.condition.text\",\n",
    "    \"weather_code\": \"current_weather.condition.code\",\n",
    "    \"wind_mph\": \"current_weather.wind_mph\",\n",
    "    \"wind_kph\": \"current_weather.wind_kph\",\n",
    "    \"pressure_mb\": \"current_weather.pressure_mb\",\n",
    "    \"precip_mm\": \"current_weather.precip_mm\",\n",
    "    \"humidity\": \"current_weather.humidity\",\n",
    "    \"cloud_cover\": \"current_weather.cloud\",\n",
    "    \"feelslike_c\": \"current_weather.feelslike_c\",\n",
    "    \"uv\": \"current_weather.uv\",\n",
    "    \"doc_created_at\": \"created_at\",\n",
    "    \"doc_last_updated\": \"last_updated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a422290",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_file = '../data/localcopy_free_weather_2025-06-03.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c84265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing documents to CSV: ../data/localcopy_free_weather_2025-06-03.csv\n",
      "Starting to fetch all documents from 'free_weather' in batches of 500...\n",
      "  Written 500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 500\n",
      "  Written 1000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 1000\n",
      "  Written 1500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 1500\n",
      "  Written 2000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 2000\n",
      "  Written 2500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 2500\n",
      "  Written 3000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 3000\n",
      "  Written 3500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 3500\n",
      "  Written 4000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 4000\n",
      "  Written 4500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 4500\n",
      "  Written 5000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 5000\n",
      "  Written 5500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 5500\n",
      "  Written 6000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 6000\n",
      "  Written 6500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 6500\n",
      "  Written 7000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 7000\n",
      "  Written 7500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 7500\n",
      "  Written 8000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 8000\n",
      "  Written 8500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 8500\n",
      "  Written 9000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 9000\n",
      "  Written 9500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 9500\n",
      "  Written 10000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 10000\n",
      "  Written 10500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 10500\n",
      "  Written 11000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 11000\n",
      "  Written 11500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 11500\n",
      "  Written 12000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 12000\n",
      "  Written 12500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 12500\n",
      "  Written 13000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 13000\n",
      "  Written 13500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 13500\n",
      "  Written 14000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 14000\n",
      "  Written 14500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 14500\n",
      "  Written 15000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 15000\n",
      "  Written 15500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 15500\n",
      "  Written 16000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 16000\n",
      "  Written 16500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 16500\n",
      "  Written 17000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 17000\n",
      "  Written 17500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 17500\n",
      "  Written 18000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 18000\n",
      "  Written 18500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 18500\n",
      "  Written 19000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 19000\n",
      "  Written 19500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 19500\n",
      "  Written 20000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 20000\n",
      "  Written 20500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 20500\n",
      "  Written 21000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 21000\n",
      "  Written 21500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 21500\n",
      "  Written 22000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 22000\n",
      "  Written 22500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 22500\n",
      "  Written 23000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 23000\n",
      "  Written 23500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 23500\n",
      "  Written 24000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 24000\n",
      "  Written 24500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 24500\n",
      "  Written 25000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 25000\n",
      "  Written 25500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 25500\n",
      "  Written 26000 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 26000\n",
      "  Written 26500 rows to CSV...\n",
      "  Fetched 500 documents in this batch. Total fetched: 26500\n",
      "  Fetched 425 documents in this batch. Total fetched: 26925\n",
      "Fetched 425 which is less than batch_size 500, assuming end of data.\n",
      "Finished fetching. Total documents processed: 26925\n",
      "Successfully wrote 26925 rows to ../data/localcopy_free_weather_2025-06-03.csv\n",
      "\n",
      "CSV generation process complete. Check '../data/localcopy_free_weather_2025-06-03.csv'.\n"
     ]
    }
   ],
   "source": [
    "all_documents_iterator = get_all_couchdb_docs_paginated(db.name, batch_size=500)\n",
    "\n",
    "write_docs_to_csv(all_documents_iterator, csv_output_file, FIELD_MAPPING)\n",
    "\n",
    "print(f\"\\nCSV generation process complete. Check '{csv_output_file}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
